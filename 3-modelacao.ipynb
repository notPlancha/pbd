{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cac8eeaa-fe81-4ff0-bb87-0e5a3f2253c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.imports import *\n",
    "from utils.start_spark import spark\n",
    "! start http://localhost:4040/jobs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62a61ac-110d-4fa6-af43-c46b2674edc0",
   "metadata": {},
   "source": [
    "Uma forma de transformar a target é, em vês da planeada \"dataframe de sessões\", teriamos uma \"dataframe de perguntas\", onde em vês de estar uma dataframe com cada linah uma sessão, estar 18 das mesmas linhas, com a diferença nas questões. Ou seja, a nossa chave primária seria o conjunto da das colunas `session_id` e `question` (categórica ou inteira), em vês de apenas ter como chave primária `session_id`.\n",
    "\n",
    "| session_id | q1 | q2 | ... | q18 | feature1 | feature2 | ... |\n",
    "|------------|----|----|-----|-----|----------|----------|-----|\n",
    "| 1          | 0  | 1  | ... | 1   | 423      | 0        | ... |\n",
    "| 2          | 1  | 0  | ... | 1   | 231      | 1        | ... |\n",
    "| 3          | 1  | 1  | ... | 1   | 345      | 1        | ... |\n",
    "\n",
    "para\n",
    "\n",
    "| session_id | question | answer | feature1 | feature2 | ... |\n",
    "|------------|----------|--------|----------|----------|-----|\n",
    "| 1          | 1        | 0      | 423      | 0        | ... |\n",
    "| 1          | 2        | 1      | 423      | 0        | ... |\n",
    "| ...        | ...      | ...    | ...      | ...      | ... |\n",
    "| 1          | 18       | 1      | 423      | 0        | ... |\n",
    "| 2          | 1        | 1      | 231      | 1        |     |\n",
    "| ...        | ...      | ...    | ...      | ...      | ... |\n",
    "\n",
    "Esta opção iria facilitar o processo de previsão, pois podemos usar métodos simples como regressão logística, usando `question` como uma feature também.\n",
    "\n",
    "Uma segunda opção seria usar classificação *multi-label*, sendo que temos vários `targets`, mas estes só têm uma carnidalidade de 2. Isto iria aumentar a complexidade do projeto. Estamos também limitados pela utilização de MLLIB do spark.\n",
    "\n",
    "A terceira opção seria treinar 18 modelos, e criar um algoritmo para escolher um modelos na precisão. Essencialmente, este é o método de *binary relevance*, sendo um dos métodos da segunda opção; no entanto, estamos a notar como opção porque não parece que o MLLIB tenha qualquer opção de *binary relevance*.\n",
    "\n",
    "Nota: da forma como os dados estão agora formatados, essencialmente é o mesmo problema se tivéssemos em vês de uma matriz de 0s e 1s, uma coluna com um array das classes onde a sessão teve certo. \n",
    "\n",
    "Decidimos usar *binary relevance*, ou seja, vamos treinar um modelo para cada pergunta.\n",
    "\n",
    "Para análise das métricas, vamos usar `MultilabelClassificationEvaluator` do MLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44389f57-eec0-4394-a376-a0204242f65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doing_features = False\n",
    "training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79119c0e-3374-4678-a437-f000ab316469",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'stages' in sys.modules: del sys.modules['stages']\n",
    "from stages import *\n",
    "\n",
    "pipeline_no_ML = Pipeline(stages = [\n",
    "    the_transformer(\n",
    "        add_id,\n",
    "        (elapsed_to_diff, \"elapsed_time\", \"elapsed_diff_ms\"),\n",
    "        (negative_to_0, \"elapsed_diff_ms\"),\n",
    "        (elapsed_to_hours, \"elapsed_time\", \"elapsed_time_h\"),\n",
    "        agg1,\n",
    "        agg2,\n",
    "        typeOfText,\n",
    "    ),\n",
    "    VectorAssembler(inputCols = [\"inv_total_time_h\", \"inv_total_time_h_0-4\", \"inv_total_time_h_5-12\"], outputCol = \"inv_time_stand_per_group\"),\n",
    "    StandardScaler(inputCol = \"inv_time_stand_per_group\", outputCol = \"inv_time_standed_per_group\", withMean = True),\n",
    "    StringIndexer(inputCol = \"type_of_script\", outputCol = \"index_of_type_of_script\"),\n",
    "    OneHotEncoder(inputCol = \"index_of_type_of_script\", outputCol=\"dummies_of_type_of_script\"), #sparse vector\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55616de8-b5f1-4d5b-ba54-3a9112690b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "if doing_features:\n",
    "    train = spark.read.parquet(\".\\\\data\\\\df_train\\\\\")\n",
    "    test = spark.read.parquet(\".\\\\data\\\\df_test\\\\\")\n",
    "\n",
    "    transformer = pipeline_no_ML.fit(train)\n",
    "    trans_train = transformer.transform(train)\n",
    "    # pivoted {\n",
    "    from files.dfs import train_labels as labels\n",
    "    if labels._isRead == False: labels.read(spark)\n",
    "    splited = labels.df \\\n",
    "        .select(\n",
    "            split(\"session_id\", \"_\").alias(\"both\"),\n",
    "            \"correct\"\n",
    "        ).select(\n",
    "            col(\"both\")[0].alias(\"session_id\"),\n",
    "            col(\"both\")[1].alias(\"question\"),\n",
    "            col(\"correct\").alias(\"isCorrect\")\n",
    "        )\n",
    "    pivoted = splited \\\n",
    "        .groupby(\"session_id\") \\\n",
    "        .pivot(\"question\") \\\n",
    "        .agg(first(\"isCorrect\"))\n",
    "    # }\n",
    "    # add pivoted\n",
    "    trans_joined = trans_train.join(pivoted, [\"session_id\"], \"left\")\n",
    "    trans_joined.drop(trans_joined.session_id)\n",
    "    trans_joined.write.mode(\"overwrite\").parquet(r\"data\\trans_train\")\n",
    "\n",
    "    trans_test = transformer.transform(test)\n",
    "    trans_test.drop(trans_test.session_id)\n",
    "    trans_test.write.mode(\"overwrite\").parquet(r\"data\\trans_test\")\n",
    "\n",
    "    trans_train = trans_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "957ede12-c782-4fe4-aec7-3582e791e03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not doing_features:\n",
    "    trans_train = spark.read.parquet(r\"data\\trans_train\")\n",
    "    trans_test = spark.read.parquet(r\"data\\trans_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "300b6d67-5239-43f5-ba92-fe2795c5b125",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ass = VectorAssembler(inputCols = [\n",
    " 'max_index',\n",
    " 'obs_opcional',\n",
    " 'obs_no_in',\n",
    " 'notebook_opens',\n",
    " 'notebook_explorer',\n",
    " 'fullscreen',\n",
    " 'hq',\n",
    " 'music',\n",
    " 'avg_elapsed_diff_ms_cutscene',\n",
    " 'avg_elapsed_diff_ms_person',\n",
    " 'avg_elapsed_diff_ms_navigate',\n",
    " 'inv_time_standed_per_group',\n",
    " 'dummies_of_type_of_script'\n",
    "    ], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1bab9d9e-d22a-4a8b-b67b-21d9c0759ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BinRel(model, features, needsProb = False,  **kwargs) -> Pipeline:\n",
    "    stages = [features]\n",
    "    for i in range(1, 19):\n",
    "        (modelParams := {\n",
    "            \"labelCol\": f\"q{i}\",\n",
    "            \"predictionCol\":f\"q{i}_pred\",\n",
    "            \"rawPredictionCol\": f\"q{i}_pred_raw\" if needsProb else None,\n",
    "            \"probabilityCol\": f\"q{i}_pred_prob\" if needsProb else None\n",
    "        }).update(kwargs)\n",
    "        model(**{a: b for a, b in modelParams.items() if b is not None})\n",
    "        stages.append(model)\n",
    "    return Pipeline(stages = stages)\n",
    "\n",
    "def train_predict(pipeline, train, test, write = True, parquetFile = None):\n",
    "    if parquetFile is None and write is True:\n",
    "        raise ValueError(\"parquet name not provided\")\n",
    "    fit = pipeline.fit(train) # train must be transformed already\n",
    "    prev = fit.transform(test)\n",
    "    if write: prev.write.mode(\"overwrite\").parquet(f\".\\\\data\\\\{parquetFile}\")\n",
    "    return (fit, prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e974feb-e1b6-48f3-b94f-fe9f58057429",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lr_Pipeline = BinRel(LogisticRegression, features_ass, \n",
    "                     needsProb = True,\n",
    "                     standardization = False\n",
    ")\n",
    "Svm_Pilenine = BinRel(LinearSVC, features_ass, standardization = False)\n",
    "DT_Pipeline = BinRel(DecisionTreeClassifier, features_ass, seed = 1)\n",
    "RF_Pipeline = BinRel(RandomForestClassifier, features_ass, seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139d1b3e-8a09-4e1b-b660-55f9706dc5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: train & evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipy310",
   "language": "python",
   "name": "ipy310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
